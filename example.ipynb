{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.sparql import return_sparql_query_results\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from pandas.io.parsers import ParserError\n",
    "import pandas as pd\n",
    "import sys\n",
    "import requests\n",
    "from requests.exceptions import SSLError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata SPARQL Query Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_sparql_engine():\n",
    "    \"\"\" \n",
    "    Step 1: create user and end point features\n",
    "   \n",
    "    user_agent = The User-Agent for the HTTP request header. \n",
    "                The default value is an autogenerated string using the SPARQLWrapper version code.\n",
    "    end_point =  SPARQL endpointâ€™s URI.\n",
    "    \n",
    "    Step 2: crete the SPARQL 'engine' by calling the SPARQLwrapper on the end_point and user_agent\n",
    "    \"\"\"\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % ( sys.version_info[0], sys.version_info[1])\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    return sparql\n",
    "\n",
    "def get_wikidata_query(engine,query_string):\n",
    "    \"\"\" \n",
    "    Step 3: We call the engine from the previous function, as well as a string of the query we want. \n",
    "    You can construct this query using the Wikidata SPARQL Interface\n",
    "    \"\"\"\n",
    "    # Call engine on query_string\n",
    "    engine.setQuery(query_string)\n",
    "    #return query results\n",
    "    engine.setReturnFormat(JSON)\n",
    "    #transform into a pandas data frame\n",
    "    results_df = pd.io.json.json_normalize(engine.query().convert()['results']['bindings'])\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Wikidata Functions for Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query goes here! \n",
    "# Feel free to erase this query and put your own\n",
    "query_string = \"\"\" SELECT ?television_program ?television_programLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "  ?television_program wdt:P31 wd:Q15416.\n",
    "}\n",
    "LIMIT 100\"\"\"\n",
    "\n",
    "# Create engine \n",
    "engine= Create_sparql_engine()\n",
    "#execute query and get results\n",
    "results = get_wikidata_query(engine, query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikimedia Rest API For Page Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Data Prep Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now that we have our query results, we need to push them into the Wikimedia Page views API in order to get \n",
    "their views information. The logic goes like this: \n",
    "1) iterate through the items we retrieved. The original case is TV Programs, so I will iterate over each name of TV shows \n",
    "to pull their views\n",
    "2) we are collecting the API results for each show in a list of json's\n",
    "3) then we extract the items from each json and concatentate it into a dataframe in pandas\n",
    "\n",
    "You will notice that there are try and except markers. This is because sometimes the information might not be available in the\n",
    "data base either because the time range is not available for it, or the page views info isnt yet available for that show.\n",
    "These instances are rare, but if you are pulling for alot of data, you might want to control for them when they come.\n",
    "\n",
    "One opportunity for the script to fail is if it ingests a result that is a code for a wikipedia item, like: Q224123\n",
    "\n",
    "Breifly, wikipedia has unique code ID's for everything in the database. Sometimes a query won't return a human readable item, \n",
    "but instead the wikicode. When this happens, it's likely that that specific wikidata page has not been sufficiently filled out, \n",
    "so no information is retrievable from it. you can read more about those codes here: \n",
    "        https://www.wikidata.org/wiki/Wikidata:Glossary\n",
    "        \n",
    " If you type results.columns you will notice that you have some additional columns.Columns like television_program.type, \n",
    "for example. These are columns that describe the type of data for each row. I found them relatively useless, so I drop them\n",
    "by selecting only the columns I want\n",
    "\n",
    "After that, I am renaming the columns I retrieved since they are not cute.\n",
    "\"\"\"\n",
    "\n",
    "# Selecting columns\n",
    "results = results[['television_programLabel.value']]\n",
    "# Renaming them \n",
    "results.columns = ['tv_show_name']\n",
    "\n",
    "\"\"\" Next, if you refer to the article I wrote, the format for inputting these wikipages into the API is with a _ in between \n",
    "each natural word spacing. So we need to add that, then we can push it to the API\"\"\"\n",
    "\n",
    "results.tv_show_name = results.tv_show_name.str.replace(' ','_',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list a pages\n",
    "list_of_pages = list(results.tv_show_name.unique())\n",
    "\n",
    "# function will accept the list of pages, and pull the pages views for them, resulting in a pandas dataframe\n",
    "def pull_pageviews_from_wikimedia(list_of_pages):\n",
    "    # empty list for all the page views data\n",
    "    dataframes = []\n",
    "    # iterate over all the pages in the list\n",
    "    for i in list_of_pages:\n",
    "        try:\n",
    "            # use requests to call on Wikimedia Server\n",
    "            index = requests.get(\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{}/daily/20200601/20200615\"\\\n",
    "                                  .format(i))\n",
    "            '''notice that in the api call  I left a {} where the page name would go. I use the .format() function to input the name\n",
    "            of the show into the api call as the script iterates through each. You can use this logic to input other parts of the \n",
    "            URL as you want. Even manipulating the end date to be today, inputting different project language values instead of en. \n",
    "            you can really explore a lot with this API. I suggest messing around with it'''\n",
    "            # collect the the json from the index server call. (similar to .text() if you are used to webscraping.)\n",
    "            result = index.json()\n",
    "        # if we hit an error, ignore and keep going since it is likely due to what I mentioned in the above cell.\n",
    "        except SSLError:\n",
    "            continue\n",
    "        try:\n",
    "            dataframes.append(pd.DataFrame(result['items']))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # once we are done iterating, we take all the jsons stored in the dataframes list and concat it to pandas df. \n",
    "    df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et voila! We have views henny :D \n",
    "page_views_df = pull_pageviews_from_wikimedia(list_of_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the resulting page views data frame, you can use that as you wish. In my own project, since I had pulled a lot of other columns from Wikidata SPARQL, I had to join the page views back to my SPARQL dataframe so that to have all the information at once. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
