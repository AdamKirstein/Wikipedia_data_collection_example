{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get-wikidata.ipynb",
      "provenance": [],
      "mount_file_id": "1l-dfvU6TpuzbTQ8siK67GWsMTeJhpZ0J",
      "authorship_tag": "ABX9TyM5XImfSPCwXS14cjNrJxRk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamKirstein/Wikipedia_data_collection_example/blob/master/lets-talk-data-walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgGqVMkyFu5D",
        "colab_type": "text"
      },
      "source": [
        "**Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBQiU7gFzd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install qwikidata --quiet\n",
        "!pip3 install SPARQLWrapper --quiet\n",
        "!pip3 install requests --quiet\n",
        "!pip3 install Wikidata --quiet\n",
        "!pip3 install wikipedia --quiet "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c870qPHGA16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from qwikidata.sparql import return_sparql_query_results \n",
        "from SPARQLWrapper import SPARQLWrapper, JSON \n",
        "from pandas.io.parsers import ParserError\n",
        "import pandas as pd\n",
        "import sys\n",
        "import requests\n",
        "from requests.exceptions import SSLError\n",
        "from datetime import datetime\n",
        "import warnings \n",
        "import wikipedia\n",
        "\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s0A9AONGZFw",
        "colab_type": "text"
      },
      "source": [
        "Start by setting up a means of establishing a connection between the Wikidata stores and your python notebook. If you are familiar with SQL Alchemy, or establishing any kind of connection with any external data base via python, it is very similar to that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7hEQ8UbGSSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Create_sparql_engine():\n",
        "    \"\"\" \n",
        "    Step 1: create user and end point features\n",
        "   \n",
        "    user_agent = The User-Agent for the HTTP request header. \n",
        "                The default value is an autogenerated string using the SPARQLWrapper version code.\n",
        "    end_point =  SPARQL endpointâ€™s URI.\n",
        "    \n",
        "    Step 2: crete the SPARQL 'engine' by calling the SPARQLwrapper on the end_point and user_agent\n",
        "    \"\"\"\n",
        "    user_agent = \"WDQS-example Python/%s.%s\" % ( sys.version_info[0], sys.version_info[1])\n",
        "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
        "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
        "    return sparql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6FX3hg5GVoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_wikidata_query(engine,query_string):\n",
        "    \"\"\" \n",
        "    Step 3: We call the engine from the previous function, as well as a string of the query we want. \n",
        "    You can construct this query using the Wikidata SPARQL Interface\n",
        "    \"\"\"\n",
        "    # Call engine on query_string\n",
        "    engine.setQuery(query_string)\n",
        "    #return query results\n",
        "    engine.setReturnFormat(JSON)\n",
        "    #transform into a pandas data frame\n",
        "    results_df = pd.io.json.json_normalize(engine.query().convert()['results']['bindings'])\n",
        "    return results_df\n",
        "\n",
        "\n",
        "\n",
        "# Page views functions \n",
        "def pull_pageviews_from_wikimedia( project, access, agent, articles, granularity, start, end ):\n",
        "  if isinstance(articles, list):\n",
        "    errors = []\n",
        "    dataframes = []\n",
        "    for article in articles:\n",
        "        index = makePageviewsRequest(project, access, agent, article, granularity, start, end)\n",
        "        index_result = index.json()\n",
        "        try:\n",
        "          dataframes.append(pd.DataFrame(index_result['items']))\n",
        "        except:\n",
        "          errors.append(article)\n",
        "    df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
        "  else: \n",
        "    index = makePageviewsRequest(project, access, agent,articles, granularity, start, end)\n",
        "    index_result = index.json()\n",
        "    df =  pd.DataFrame(index_result['items'])\n",
        "  return df, errors\n",
        "\n",
        "\n",
        "\n",
        "def makePageviewsRequest(project, access, agent,articles, granularity, start, end ):\n",
        "  index = requests.get(\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{}.wikipedia.org/{}/{}/{}/{}/{}/{}\"\\\n",
        "                                  .format(project, access, agent,articles, granularity, start, end ))\n",
        "  return index \n",
        "\n",
        "\n",
        "# assets functions\n",
        "def pull_page_description(wikiFilmList):\n",
        "    wikiFilmList = [(\" \".join(i.split(\"_\")))for i in wikiFilmList]\n",
        "    summaries_dict = {}\n",
        "    for i in wikiFilmList:\n",
        "      try:\n",
        "        summaries_dict[i] = wikipedia.page(i).content\n",
        "      except:\n",
        "        print(i)\n",
        "    return summaries_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl1wfOXV4XAS",
        "colab_type": "text"
      },
      "source": [
        "# **Section 1: Interface with Wikidata**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5NpnxyNo7kh",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1 Query Anatomy \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#select our columns!\n",
        "#Column variables will always start with '?' before the name\n",
        "# we need to specify a item (film, in this case) and a label value. \n",
        "# w/o the label value, we wont be able to read the results. \n",
        "\n",
        "\n",
        "SELECT DISTINCT ?film ?filmLabel ?date ?alias \n",
        "WHERE{\n",
        "  # i want all instances of film (gimme all dem films) \n",
        "\n",
        "  ?film wdt:P31 wd:Q11424;\n",
        "\n",
        "        #now we are targeting the Published date node and saying\n",
        "        # i need the country value that you have living inside of you please\n",
        "        #you're also saying, in addition to that, i need the date value too \n",
        "\n",
        "        #publish date node -> published location -> United States value\n",
        "        p:P577 [ pq:P291 wd:Q30; ps:P577 ?date].\n",
        "\n",
        "  # in addition to the film name as it appears in wikidata, we also want to select the version of the title\n",
        "  # as it appears on Wikipedia. We do this to have specificity in our data as some names are shared across multiple\n",
        "  # subjects. \n",
        "\n",
        "  # ?alias = film name as it appears on wikipedia\n",
        "  ?article schema:about  ?film ; schema:isPartOf <https://en.wikipedia.org/> ;  schema:name ?alias .\n",
        "\n",
        "  # always include this in your query. it determines the language that your output will be in \n",
        "\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE], en\". }\n",
        "}\n",
        "\n",
        "ORDER BY DESC(?date)\n",
        "limit 100\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yayqob4m7SJW",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1.5 Pseudocode For Query\n",
        "\n",
        "\n",
        "---\n",
        "To remind you, the query we constructed is saying: \n",
        "\n",
        "give me any film, published in the United States, date of publication in the US, and the way the name of the film appears on wikipedia.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWIXqSINpKdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### 1.2 Use this for your query \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "copy the below, then go to: https://query.wikidata.org/\n",
        "\n",
        "```\n",
        "SELECT DISTINCT ?film ?filmLabel ?date ?alias\n",
        "WHERE\n",
        "{\n",
        "  ?film wdt:P31 wd:Q11424;\n",
        "        p:P577 [ pq:P291 wd:Q30; ps:P577 ?date].  \n",
        "  ?article schema:about  ?film ; schema:isPartOf <https://en.wikipedia.org/> ;  schema:name ?alias .\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE], en\". }\n",
        "}\n",
        "ORDER BY DESC(?date)\n",
        "limit 100\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnyQ-W5U7yXx",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.5 Paste Query in query_string quotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSqbsXOGX2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_string = \"\"\" erase me and paste query here plz (don't erase the quotation marks tho) \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1usk5DptHft_",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Initialize the connection between python and Wikidata and retrieve result\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlIsuSs0HurC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "engine= Create_sparql_engine()\n",
        "# give the results function the engine (access to the Wikidata Base ) and your query to execute on \n",
        "results = get_wikidata_query(engine, query_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxSVUwYm3oHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zyd6gmkAUAE",
        "colab_type": "text"
      },
      "source": [
        "## 1.3.5 Clean Up Results\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ArtttfAZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Select the columns we want\n",
        "wikidata_results = results[['alias.value','date.value', 'film.value']]\n",
        "# # # rename them to make them cute\n",
        "wikidata_results.columns = ['film', 'release_date', 'wikipedia_code']\n",
        "# # #clean up wikipedia code\n",
        "wikidata_results.wikipedia_code = [(i.split('/')[-1])for i in wikidata_results.wikipedia_code]\n",
        "# # #format dates\n",
        "wikidata_results.release_date= pd.to_datetime(wikidata_results.release_date).dt.strftime(\"%Y-%m-%d\")\n",
        "# # # format input to match what is required by the Meta-Wiki API input\n",
        "wikidata_results['meta_wiki_format'] = wikidata_results.film.str.replace(' ',\"_\")\n",
        "wikidata_results.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiVUp5yh3lgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikidata_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8OTOO8UOSkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_film_list = wikidata_results.meta_wiki_format.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDfOyBRh4UaS",
        "colab_type": "text"
      },
      "source": [
        "# Section 2: **Meta Wiki Pageviews API**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdsYrNVRIZNC",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 API Anatomy \n",
        "### **The Meta-Wiki Rest API takes 7 arguments:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   project - language of wikipedia \n",
        "*   access - type of access (Desktop, Mobile, etc)\n",
        "\n",
        "*   agent - type of user. (Bot, real user, api call, etc)\n",
        "*   article - the name of the wikipedia page. IT MUST BE IN FORMAT My_Wikipedia_Page (with underscores filling natural word spacing)\n",
        "\n",
        "\n",
        "*   granularity - daily, monthly, hourly (tbd)\n",
        "*   start time - earliest date you want your query to start at**\n",
        "\n",
        "\n",
        "*   end date  - the max date you want your query to consider**\n",
        "\n",
        "\n",
        "\n",
        "**Date formats must be YYYYMMDD or YYYYMMDDHH\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBJrilDG8TOx",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 **Retrieve Page Views From API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdGQ2qFZZ0Z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_wiki_pageviews,errors = pull_pageviews_from_wikimedia(project = 'en',\n",
        "                                     access= 'all-access',\n",
        "                                     agent= 'user',\n",
        "                                     articles= wiki_film_list,\n",
        "                                     granularity='daily', \n",
        "                                     start='20190101',\n",
        "                                     end='20200728')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUDHc1nYmQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_wiki_pageviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrzVEO38gwO",
        "colab_type": "text"
      },
      "source": [
        "# Section 3: **MediaWiki Content API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc7LDKgaLb2D",
        "colab_type": "text"
      },
      "source": [
        "# **Media Wiki Page Content**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBfc6DPPY04J",
        "colab_type": "text"
      },
      "source": [
        "This API will return to you the page of the wikipedia topic into which you feed it. The required format of string input is the opposite to the pageviews. (ie. no underscores between natural spacing. \n",
        "\n",
        "input : My Film Name\n",
        "output: wikipedia page (text)\n",
        "\n",
        "You might encounter an error wherein your input does not return an output. \n",
        "it's at this point where you will wnt to return to the wikidata page of that item to look t it's \"Also Known As\" section found by clicking the \"In More Languages\" drop down beneath the title of the page. you can target and collect these AKA values in your query, and then concieve of a script that tries each one if the initial one fails, until you get a result. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrFpxxltZnxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b4c3053-369d-44d5-be49-3b6273ab895a"
      },
      "source": [
        "test = pull_page_description(wiki_film_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What About Love (film)\n",
            "Ip Man 4: The Finale\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf_amBvJedGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(test.values())[6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anUABk6A6S7u",
        "colab_type": "text"
      },
      "source": [
        "**END**"
      ]
    }
  ]
}